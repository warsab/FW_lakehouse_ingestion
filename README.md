# FreedomWings FabricÂ SparkÂ UtilsÂ ğŸš€

Load Î”â€‘tables â¬‡ï¸Â â†’ Register Temp Views ğŸª„Â â†’ Query with SparkÂ SQL ğŸ“Š

A lightweight utility toolbox for FreedomWings teams working on MicrosoftÂ Fabric / AzureÂ Lakehouse projects.create_spark_tables quickly reads Delta tables from your Lakehouse, registers them as temporary views, and lets you run any SQL script against themâ€”all in a single call.

## ğŸ§°Â Features

Oneâ€‘liner ingestion of one or many Delta tables into Spark.

Automatic tempâ€‘view registration âœ¨â€”no manual createOrReplaceTempView calls.

Simple debugging prints for workspace, layer, and row counts.

Works across base / raw / enriched layers.

Returns a readyâ€‘toâ€‘use DataFrame so you can chain transformations or analytics right away.

## ğŸš€Â QuickÂ Start

Prerequisites

| Requirement | MinÂ Version | Notes |
|-------------|------------|-------|
| Python      | 3.8+        | Tested on 3.10 |
| PySpark     | 3.4+        | Cluster / Fabric runtime should match |
| Access      | âœ…          | Fabric workspace & Lakehouse + Delta tables |


##  Access

Â âœ… Fabric workspace & Lakehouse +Â Delta tables

## Installation

In a Databricks or Fabric notebook cell
(No PyPI package yetâ€”simply copy the .py file or gitâ€‘clone this repo)
!git clone https://github.com/yourâ€‘org/FW_fabric_spark_utils.git
%pip install pyspark  # if not already available in the runtime

## ğŸ”§Â Usage Example

``` from create_spark_tables import create_spark_tables

sql = """
SELECT user_id, created_at
FROM name_of_dbo
WHERE date >= '2025â€‘01â€‘01'
"""

df = create_spark_tables(
    workspace="name_of_workspace",
    lakehouse_layer="base",
    table_names=["your_table_name"],
    sql_script=sql,
)

df.show(5)
```

## ğŸ› ï¸Â FunctionÂ Reference

create_spark_tables(workspace, lakehouse_layer, table_names, sql_script) â†’ pyspark.sql.DataFrame

| Parameter       | Type        | Description                                                                 |
|----------------|-------------|-----------------------------------------------------------------------------|
| workspace       | str         | Fabric workspace name (e.g. `Financial_Lakhouse`)                     |
| lakehouse_layer | str         | Lakehouse layer: `base`, `raw`, or `enriched` (caseâ€‘insensitive)            |
| table_names     | list[str]   | One or more Delta table names in the layer                                 |
| sql_script      | str         | Any Spark SQL query that references the registered views                   |


Returns: A SparkÂ DataFrame with the query result.

Raises: Propagates any exceptions that occur during table load or SQL execution.

## ğŸš¨Â Error Handling & Debugging

Each table load prints its row count and schema (optional) so you can verify data quickly.

Errors per table are logged but donâ€™t stop the loopâ€”handy when youâ€™re loading many views.

A final count of the result set is printed after the SQL executes.

## ğŸ¤Â Contributing

Fork â¡ï¸Â clone â¡ï¸Â create a feature branch.

Follow PEPÂ 8 and include docstrings & type hints.

Submit a pull request with a clear description & example.

## ğŸ“„Â License

Distributed under the MIT License. See LICENSE for more information.

ğŸ‘‹Â Contact & Support

Made with â¤ï¸Â by FreedomWings. If you find a bug or have a feature request, please open anÂ issue.

